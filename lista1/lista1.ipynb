{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6568bf6c",
   "metadata": {},
   "source": [
    "3. Faça uma tabela com média e desvio-padrão das variáveis. Faça outra tabela com a correlação entre S e IQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "449cf12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('grilic.xls')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78ec3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = df.agg(['mean', 'std'], numeric_only=True)\n",
    "\n",
    "correlacao_S_IQ = df['S'].corr(df['IQ'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78a5cf",
   "metadata": {},
   "source": [
    "4. Apresente uma análise da escolha dos instrumentos para resolver os problemas de viés de variável omitida e erro-nas-variáveis. Inclua nesta análise a regressão de primeiro estágio que achar pertinente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0947f510",
   "metadata": {},
   "source": [
    "Instrumentos usados poderiam ser KWW, MED\n",
    "\n",
    "Os instrumentos devem ser correlacionados com as variáveis endógenas (IQ e S) e devem ser exógenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9829f3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "linhas_interesse = ['S', 'IQ']\n",
    "colunas_interesse = ['MED', 'KWW', 'MRT', 'AGE']\n",
    "\n",
    "# Calcular a matriz de correlação completa para todas as colunas relevantes\n",
    "# Primeiro, criamos uma lista com todas as colunas que participarão\n",
    "colunas_para_corr_completa = list(set(linhas_interesse + colunas_interesse)) # set() para evitar duplicatas\n",
    "matriz_corr_completa = df[colunas_para_corr_completa].corr(numeric_only=True)\n",
    "\n",
    "# Selecionar apenas as correlações de interesse\n",
    "matriz_corr_especifica = matriz_corr_completa.loc[linhas_interesse, colunas_interesse]\n",
    "\n",
    "correlacao_iv = pd.DataFrame(matriz_corr_especifica).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8384608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      S   R-squared:                       0.496\n",
      "Model:                            OLS   Adj. R-squared:                  0.488\n",
      "Method:                 Least Squares   F-statistic:                     61.11\n",
      "Date:                Thu, 12 Jun 2025   Prob (F-statistic):          2.58e-102\n",
      "Time:                        19:10:17   Log-Likelihood:                -1423.9\n",
      "No. Observations:                 758   AIC:                             2874.\n",
      "Df Residuals:                     745   BIC:                             2934.\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          3.9929      0.563      7.094      0.000       2.888       5.098\n",
      "RNS           -0.1541      0.135     -1.143      0.253      -0.419       0.110\n",
      "SMSA          -0.0473      0.131     -0.360      0.719      -0.305       0.211\n",
      "MED            0.1463      0.022      6.529      0.000       0.102       0.190\n",
      "KWW            0.0862      0.009      9.124      0.000       0.068       0.105\n",
      "MRT           -0.2431      0.134     -1.820      0.069      -0.505       0.019\n",
      "AGE            0.1642      0.027      6.038      0.000       0.111       0.218\n",
      "YEAR_67        0.8839      0.232      3.802      0.000       0.428       1.340\n",
      "YEAR_68        1.2900      0.217      5.940      0.000       0.864       1.716\n",
      "YEAR_69        1.4356      0.208      6.915      0.000       1.028       1.843\n",
      "YEAR_70        1.7802      0.231      7.713      0.000       1.327       2.233\n",
      "YEAR_71        1.4069      0.214      6.562      0.000       0.986       1.828\n",
      "YEAR_73        2.8258      0.181     15.623      0.000       2.471       3.181\n",
      "==============================================================================\n",
      "Omnibus:                        7.718   Durbin-Watson:                   1.968\n",
      "Prob(Omnibus):                  0.021   Jarque-Bera (JB):                9.153\n",
      "Skew:                          -0.140   Prob(JB):                       0.0103\n",
      "Kurtosis:                       3.460   Cond. No.                         440.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "year_column = 'YEAR' # << AJUSTE AQUI SE O NOME DA COLUNA DE ANO FOR DIFERENTE (ex: 'YEAR')\n",
    "\n",
    "\n",
    "year_dummies = pd.get_dummies(df[year_column], prefix=year_column, drop_first=True, dtype=int)\n",
    "\n",
    "# Juntar as dummies ao DataFrame (ou a uma cópia para não modificar o original)\n",
    "df_with_dummies = pd.concat([df, year_dummies], axis=1)\n",
    "\n",
    "dependent_var = 'S'\n",
    "original_independent_vars = ['RNS', 'SMSA', 'MED', 'KWW', 'MRT', 'AGE']\n",
    "# Adicionar os nomes das colunas dummy à lista de variáveis independentes\n",
    "all_independent_vars = original_independent_vars + year_dummies.columns.tolist()\n",
    "\n",
    "# Verificar se todas as colunas necessárias existem no df_with_dummies\n",
    "# (Verifica no df_with_dummies, que agora contém as dummies e as originais)\n",
    "missing_cols_check = [col for col in all_independent_vars if col not in df_with_dummies.columns]\n",
    "if dependent_var not in df_with_dummies.columns: # Checa também a variável dependente\n",
    "missing_cols_check.append(dependent_var)\n",
    "\n",
    "\n",
    "Y = df_with_dummies[dependent_var].astype(float) # Garante que Y seja float\n",
    "X = df_with_dummies[all_independent_vars].astype(float) # Garante que X seja float\n",
    "X = sm.add_constant(X) # Adicionar o intercepto\n",
    "\n",
    "# Lidar com NaNs combinando Y e X e depois removendo linhas com qualquer NaN\n",
    "# Isso garante que Y e X tenham o mesmo número de observações válidas.\n",
    "combined_data = pd.concat([Y, X], axis=1).dropna()\n",
    "\n",
    "if combined_data.shape[0] < X.shape[1]: # Checar se há dados suficientes após dropna\n",
    "    print(f\"Dados insuficientes para realizar a regressão após remover NaNs.\")\n",
    "    print(f\"Observações restantes: {combined_data.shape[0]}, Regressores (incl. intercepto): {X.shape[1]}\")\n",
    "else:\n",
    "    Y_clean = combined_data[dependent_var]\n",
    "    # X_clean deve ser selecionado do combined_data para manter o alinhamento após dropna\n",
    "    # e para garantir que contenha as colunas corretas (incluindo 'const' e as dummies)\n",
    "    X_clean = combined_data.drop(columns=[dependent_var]) \n",
    "    \n",
    "    model = sm.OLS(Y_clean, X_clean)\n",
    "    results = model.fit()\n",
    "    print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3179c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     IQ   R-squared:                       0.251\n",
      "Model:                            OLS   Adj. R-squared:                  0.239\n",
      "Method:                 Least Squares   F-statistic:                     20.82\n",
      "Date:                Thu, 12 Jun 2025   Prob (F-statistic):           1.06e-39\n",
      "Time:                        19:14:29   Log-Likelihood:                -2944.9\n",
      "No. Observations:                 758   AIC:                             5916.\n",
      "Df Residuals:                     745   BIC:                             5976.\n",
      "Df Model:                          12                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         76.0966      4.187     18.176      0.000      67.878      84.315\n",
      "RNS           -3.0838      1.002     -3.077      0.002      -5.051      -1.116\n",
      "SMSA          -0.0375      0.977     -0.038      0.969      -1.955       1.880\n",
      "MED            0.6390      0.167      3.834      0.000       0.312       0.966\n",
      "KWW            0.6729      0.070      9.578      0.000       0.535       0.811\n",
      "MRT           -1.0517      0.994     -1.059      0.290      -3.002       0.899\n",
      "AGE           -0.3528      0.202     -1.744      0.082      -0.750       0.044\n",
      "YEAR_67        2.7992      1.729      1.619      0.106      -0.596       6.194\n",
      "YEAR_68        3.3086      1.615      2.048      0.041       0.138       6.480\n",
      "YEAR_69        5.2912      1.544      3.426      0.001       2.260       8.323\n",
      "YEAR_70        9.9276      1.717      5.783      0.000       6.557      13.298\n",
      "YEAR_71        8.1922      1.595      5.136      0.000       5.061      11.323\n",
      "YEAR_73       10.9101      1.345      8.109      0.000       8.269      13.551\n",
      "==============================================================================\n",
      "Omnibus:                       22.303   Durbin-Watson:                   2.019\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               25.352\n",
      "Skew:                          -0.360   Prob(JB):                     3.13e-06\n",
      "Kurtosis:                       3.533   Cond. No.                         440.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "dependent_var = 'IQ'\n",
    "original_independent_vars = ['RNS', 'SMSA', 'MED', 'KWW', 'MRT', 'AGE']\n",
    "\n",
    "missing_cols_check.append(dependent_var)\n",
    "\n",
    "\n",
    "Y = df_with_dummies[dependent_var].astype(float) # Garante que Y seja float\n",
    "X = df_with_dummies[all_independent_vars].astype(float) # Garante que X seja float\n",
    "X = sm.add_constant(X) # Adicionar o intercepto\n",
    "\n",
    "\n",
    "combined_data = pd.concat([Y, X], axis=1).dropna()\n",
    "\n",
    "if combined_data.shape[0] < X.shape[1]: # Checar se há dados suficientes após dropna\n",
    "    print(f\"Dados insuficientes para realizar a regressão após remover NaNs.\")\n",
    "    print(f\"Observações restantes: {combined_data.shape[0]}, Regressores (incl. intercepto): {X.shape[1]}\")\n",
    "else:\n",
    "    Y_clean = combined_data[dependent_var]\n",
    "    # X_clean deve ser selecionado do combined_data para manter o alinhamento após dropna\n",
    "    # e para garantir que contenha as colunas corretas (incluindo 'const' e as dummies)\n",
    "    X_clean = combined_data.drop(columns=[dependent_var]) \n",
    "    \n",
    "    model = sm.OLS(Y_clean, X_clean)\n",
    "    results = model.fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a840620",
   "metadata": {},
   "source": [
    "5. Griliches também argumenta que escolaridade pode ser endógena. Qual o argumento dele? Estime a equação salarial por 2SLS assumindo IQ e S como endógenas. O que acontece com o coeciente de escolaridade? Como você pode explicar a diferença entre sua estimativa 2SLS de S e a estimativa anterior (sem tratar S como endógena)? Calcule a estatística de Sargan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd224c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
