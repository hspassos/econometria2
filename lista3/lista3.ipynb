{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b11557ce",
   "metadata": {},
   "source": [
    "1. Estime os parâmetros $\\beta_k$, $\\beta_l$ por GMM utilizando o estimador apresentado acima. Escreva um programa em Matlab utilizando o solver fminsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b746bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chilean = pd.read_csv('chilean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a03f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.824\n",
      "Model:                            OLS   Adj. R-squared:                  0.823\n",
      "Method:                 Least Squares   F-statistic:                     843.9\n",
      "Date:                Fri, 20 Jun 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:57:44   Log-Likelihood:                -2369.4\n",
      "No. Observations:                2544   AIC:                             4769.\n",
      "Df Residuals:                    2529   BIC:                             4856.\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.2094      0.140     65.592      0.000       8.934       9.485\n",
      "k              0.0015      0.019      0.078      0.937      -0.036       0.039\n",
      "l              1.0511      0.047     22.236      0.000       0.958       1.144\n",
      "l_4           -0.0132      0.005     -2.724      0.006      -0.023      -0.004\n",
      "k_l_3          0.0096      0.005      1.788      0.074      -0.001       0.020\n",
      "k2_l2         -0.0012      0.002     -0.639      0.523      -0.005       0.003\n",
      "k3_l          -0.0006      0.000     -2.216      0.027      -0.001   -7.06e-05\n",
      "k_4            0.0002   1.94e-05      8.620      0.000       0.000       0.000\n",
      "m_4            0.0055      0.001      6.179      0.000       0.004       0.007\n",
      "k_m_3         -0.0097      0.001     -7.630      0.000      -0.012      -0.007\n",
      "k2_m2          0.0049      0.001      7.245      0.000       0.004       0.006\n",
      "k3_m          -0.0008      0.000     -5.480      0.000      -0.001      -0.001\n",
      "m_l_3          0.0092      0.003      3.152      0.002       0.003       0.015\n",
      "m2_l2         -0.0160      0.004     -4.533      0.000      -0.023      -0.009\n",
      "m3_l           0.0082      0.002      4.881      0.000       0.005       0.012\n",
      "==============================================================================\n",
      "Omnibus:                       79.225   Durbin-Watson:                   0.700\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              169.446\n",
      "Skew:                           0.178   Prob(JB):                     1.60e-37\n",
      "Kurtosis:                       4.213   Cond. No.                     3.52e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.52e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      " Intercepto do primeiro estágio: 9.209416673947127\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "df = chilean.rename(columns={'Y': 'y', 'sX': 'k', 'pX': 'm', 'fX1': 'fx1', 'fX2': 'fx2'}).copy()\n",
    "\n",
    "\n",
    "# l = ln(exp(fx1) + exp(fx2))\n",
    "df['l'] = np.log(np.exp(df['fx1']) + np.exp(df['fx2']))\n",
    "\n",
    "# Selecionar variáveis relevantes e criar uma cópia explícita\n",
    "df_panel = df[['idvar', 'timevar', 'y', 'k', 'l', 'm']].copy()\n",
    "\n",
    "# Ordenar dados para lagging\n",
    "df_panel.sort_values(['idvar', 'timevar'], inplace=True)\n",
    "\n",
    "# Primeiro Estágio: OLS\n",
    "\n",
    "# Criar termos polinomiais de 4ª ordem para f^-1(k, l, m)\n",
    "X_stage1 = pd.DataFrame(index=df_panel.index)\n",
    "X_stage1['const'] = 1\n",
    "X_stage1['k'] = df_panel['k']\n",
    "X_stage1['l'] = df_panel['l']\n",
    "\n",
    "# $\\sum_{i=0}^4 \\beta k^i * l^{4-i}$\n",
    "X_stage1['l_4'] = df_panel['l']**4\n",
    "X_stage1['k_l_3'] = df_panel['k'] * (df_panel['l']**3)\n",
    "X_stage1['k2_l2'] = (df_panel['k']**2) * (df_panel['l']**2)\n",
    "X_stage1['k3_l'] = (df_panel['k']**3) * df_panel['l']\n",
    "X_stage1['k_4'] = df_panel['k']**4\n",
    "\n",
    "# m^4 + k*m^3 + k^2*m^2 + k^3*m (k^4 já foi definido)\n",
    "X_stage1['m_4'] = df_panel['m']**4\n",
    "X_stage1['k_m_3'] = df_panel['k'] * (df_panel['m']**3)\n",
    "X_stage1['k2_m2'] = (df_panel['k']**2) * (df_panel['m']**2)\n",
    "X_stage1['k3_m'] = (df_panel['k']**3) * df_panel['m']\n",
    "\n",
    "# m*l^3 + m^2*l^2 + m^3*l (l^4 e m^4 já foram definidos)\n",
    "X_stage1['m_l_3'] = df_panel['m'] * (df_panel['l']**3)\n",
    "X_stage1['m2_l2'] = (df_panel['m']**2) * (df_panel['l']**2)\n",
    "X_stage1['m3_l'] = (df_panel['m']**3) * df_panel['l']\n",
    "\n",
    "y_stage1 = df_panel['y']\n",
    "\n",
    "\n",
    "data_stage1_full = pd.concat([y_stage1, X_stage1], axis=1)\n",
    "data_stage1_clean = data_stage1_full.dropna()\n",
    "\n",
    "y_stage1_clean = data_stage1_clean['y']\n",
    "X_stage1_clean = data_stage1_clean.drop(columns=['y'])\n",
    "\n",
    "model_stage1 = sm.OLS(y_stage1_clean, X_stage1_clean)\n",
    "results_stage1 = model_stage1.fit()\n",
    "print(results_stage1.summary())\n",
    "\n",
    "beta_0_tilde = results_stage1.params['const']\n",
    "print(f\"\\n Intercepto do primeiro estágio: {beta_0_tilde}\")\n",
    "\n",
    "# Calcular Phi_hat e adicionar de volta ao df_panel original usando o índice\n",
    "df_panel['Phi_hat'] = np.nan\n",
    "df_panel.loc[X_stage1_clean.index, 'Phi_hat'] = results_stage1.predict(X_stage1_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220fa5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segundo Estágio - GMM:\n",
      "Optimization successful.\n",
      "   Final GMM objective function value: 6.993736e-12\n",
      "   Estimated beta_k: 0.3618\n",
      "   Estimated beta_l: -0.1739\n"
     ]
    }
   ],
   "source": [
    "# Segundo Estágio: GMM\n",
    "\n",
    "# Criar variáveis defasadas (k_{t-1}, l_{t-1}, Phi_hat_{t-1})\n",
    "df_panel['k_lag1'] = df_panel.groupby('idvar')['k'].shift(1)\n",
    "df_panel['l_lag1'] = df_panel.groupby('idvar')['l'].shift(1)\n",
    "df_panel['Phi_hat_lag1'] = df_panel.groupby('idvar')['Phi_hat'].shift(1)\n",
    "\n",
    "# Selecionar apenas as colunas necessárias para o GMM\n",
    "df_stage2 = df_panel[['y', 'k', 'l', 'Phi_hat_lag1', 'k_lag1', 'l_lag1']].dropna()\n",
    "\n",
    "y_jt = df_stage2['y'].values\n",
    "k_jt = df_stage2['k'].values\n",
    "l_jt = df_stage2['l'].values\n",
    "Phi_hat_t_minus_1 = df_stage2['Phi_hat_lag1'].values\n",
    "k_jt_minus_1 = df_stage2['k_lag1'].values\n",
    "l_jt_minus_1 = df_stage2['l_lag1'].values\n",
    "\n",
    "rho_hat = 0.9528 # Valor fixo conforme a questão\n",
    "\n",
    "# Função de momentos para GMM\n",
    "def gmm_moments(params_gmm):\n",
    "    beta_k, beta_l = params_gmm\n",
    "    \n",
    "    # Resíduo u_jt da Equação (6)\n",
    "    # u_jt = y_jt - beta_0_tilde - beta_k*k_jt - beta_l*l_jt - rho_hat * (Phi_hat_{t-1} - beta_0_tilde - beta_k*k_{t-1} - beta_l*l_{t-1})\n",
    "    \n",
    "    term_current_prod = y_jt - beta_0_tilde - beta_k * k_jt - beta_l * l_jt\n",
    "    term_lagged_prod_expectation = Phi_hat_t_minus_1 - beta_0_tilde - beta_k * k_jt_minus_1 - beta_l * l_jt_minus_1\n",
    "    \n",
    "    u_jt = term_current_prod - rho_hat * term_lagged_prod_expectation\n",
    "    \n",
    "    # Condições de momento E[u_jt * Z_jt] = 0\n",
    "    # Z_jt = [k_jt, l_jt_minus_1]\n",
    "    moment1 = np.mean(u_jt * k_jt)           # Instrumento: k_jt\n",
    "    moment2 = np.mean(u_jt * l_jt_minus_1)   # Instrumento: l_jt-1\n",
    "    \n",
    "    return np.array([moment1, moment2])\n",
    "\n",
    "# Função objetivo para GMM (soma dos quadrados dos momentos)\n",
    "# Para GMM exatamente identificado, a matriz de ponderação W pode ser a identidade.\n",
    "def gmm_objective(params_gmm):\n",
    "    moments = gmm_moments(params_gmm)\n",
    "    return np.sum(moments**2) # Equivalente a m'I m\n",
    "\n",
    "# Valores iniciais para beta_k, beta_l (podem vir de um OLS simples ou serem arbitrários razoáveis)\n",
    "# Usando os coeficientes de k e l do primeiro estágio como ponto de partida.\n",
    "# Valores iniciais comuns para elasticidades.\n",
    "initial_params_gmm = [0.1, 0.6] \n",
    "\n",
    "print(\"\\nSegundo Estágio - GMM:\")\n",
    "# Otimização usando Nelder-Mead (análogo ao fminsearch do Matlab)\n",
    "result_gmm = minimize(gmm_objective, initial_params_gmm, method='Nelder-Mead')\n",
    "\n",
    "print(\"Optimization successful.\")\n",
    "beta_k_gmm, beta_l_gmm = result_gmm.x\n",
    "print(f\"   Final GMM objective function value: {result_gmm.fun:.6e}\")\n",
    "print(f\"   Estimated beta_k: {beta_k_gmm:.4f}\")\n",
    "print(f\"   Estimated beta_l: {beta_l_gmm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf18ef1",
   "metadata": {},
   "source": [
    "2. Calcule a estatística de Wald (F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c573dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste de Wald para significância conjunta de beta_k e beta_l (H0: beta_k=0, beta_l=0):\n",
      "   Número de observações (GMM): 2047\n",
      "   Matriz G_hat:\n",
      "[[-7.68205934 -1.58854136]\n",
      " [-1.89250106 -0.34947153]]\n",
      "   Matriz Omega_hat:\n",
      "[[59.6826729  14.58022482]\n",
      " [14.58022482  3.83646026]]\n",
      "   Matriz de Variância-Covariância (V_cov_beta_hat):\n",
      "[[ 0.0036915  -0.01564909]\n",
      " [-0.01564909  0.0765797 ]]\n",
      "   Estatística de Wald (Chi^2(2)): 215.9948\n",
      "   Estatística F(2, 2045): 107.9974\n",
      "   P-valor (F): 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f as f_dist\n",
    "import numpy as np\n",
    "\n",
    "N_gmm = len(y_jt) # Número de observações na estimação GMM\n",
    "K_params = 2      # Número de parâmetros estimados (beta_k, beta_l)\n",
    "q_restrictions = 2 # Número de restrições (beta_k=0, beta_l=0)\n",
    "\n",
    "# 1. Calcular as derivadas parciais de u_jt em relação a beta_k e beta_l\n",
    "# u_jt = y_jt - beta_0_tilde - beta_k*k_jt - beta_l*l_jt - rho_hat * (Phi_hat_t_minus_1 - beta_0_tilde - beta_k*k_jt_minus_1 - beta_l*l_jt_minus_1)\n",
    "# d(u_jt)/d(beta_k) = -k_jt + rho_hat * k_jt_minus_1\n",
    "# d(u_jt)/d(beta_l) = -l_jt + rho_hat * l_jt_minus_1\n",
    "\n",
    "du_dbk = -k_jt + rho_hat * k_jt_minus_1\n",
    "du_dbl = -l_jt + rho_hat * l_jt_minus_1\n",
    "\n",
    "# 2. Calcular G_hat (Jacobiano médio das condições de momento)\n",
    "# Instrumentos Z_jt = [k_jt, l_jt_minus_1]\n",
    "# G_hat é uma matriz 2x2, onde G_hat[i,j] = mean( Z_i * d(u_jt)/d(beta_j) )\n",
    "# G_hat = E[Z_jt * (du/d_beta)']\n",
    "# G_hat_row1 = [mean(k_jt * du_dbk), mean(k_jt * du_dbl)]\n",
    "# G_hat_row2 = [mean(l_jt_minus_1 * du_dbk), mean(l_jt_minus_1 * du_dbl)]\n",
    "# No entanto, a formulação padrão para V_cov usa G = E[ (du/d_beta) * Z' ]\n",
    "# G_hat_col1 = [mean(du_dbk * k_jt), mean(du_dbk * l_jt_minus_1)]\n",
    "# G_hat_col2 = [mean(du_dbl * k_jt), mean(du_dbl * l_jt_minus_1)]\n",
    "\n",
    "G_hat_11 = np.mean(du_dbk * k_jt)\n",
    "G_hat_12 = np.mean(du_dbl * k_jt)\n",
    "G_hat_21 = np.mean(du_dbk * l_jt_minus_1)\n",
    "G_hat_22 = np.mean(du_dbl * l_jt_minus_1)\n",
    "G_hat = np.array([[G_hat_11, G_hat_12], [G_hat_21, G_hat_22]])\n",
    "\n",
    "\n",
    "# 3. Calcular resíduos u_jt_hat com os parâmetros GMM estimados\n",
    "term_current_prod_hat = y_jt - beta_0_tilde - beta_k_gmm * k_jt - beta_l_gmm * l_jt\n",
    "term_lagged_prod_expectation_hat = Phi_hat_t_minus_1 - beta_0_tilde - beta_k_gmm * k_jt_minus_1 - beta_l_gmm * l_jt_minus_1\n",
    "u_jt_hat = term_current_prod_hat - rho_hat * term_lagged_prod_expectation_hat\n",
    "\n",
    "# 4. Calcular contribuições individuais para os momentos m_it = u_jt_hat * Z_it\n",
    "# m_t = [u_jt_hat * k_jt, u_jt_hat * l_jt_minus_1]\n",
    "m1_t_hat = u_jt_hat * k_jt\n",
    "m2_t_hat = u_jt_hat * l_jt_minus_1\n",
    "\n",
    "# 5. Calcular Omega_hat (matriz de variância-covariância dos momentos m_t)\n",
    "# Omega_hat[i,j] = mean(m_it * m_jt)\n",
    "Omega_hat_11 = np.mean(m1_t_hat**2)\n",
    "Omega_hat_12 = np.mean(m1_t_hat * m2_t_hat) # = mean(m2_t_hat * m1_t_hat)\n",
    "Omega_hat_22 = np.mean(m2_t_hat**2)\n",
    "Omega_hat = np.array([[Omega_hat_11, Omega_hat_12], [Omega_hat_12, Omega_hat_22]])\n",
    "\n",
    "# 6. Calcular a matriz de variância-covariância dos estimadores GMM\n",
    "# Para GMM exatamente identificado (W=I), V_cov(beta_hat) = (1/N) * (G_hat' * G_hat)^-1 * (G_hat' * Omega_hat * G_hat) * (G_hat' * G_hat)^-1\n",
    "# Ou, de forma mais simples para o caso exatamente identificado: V_cov(beta_hat) = (1/N) * (G_hat^-1) * Omega_hat * (G_hat'^-1)\n",
    "\n",
    "G_hat_inv = np.linalg.inv(G_hat)\n",
    "V_cov_beta_hat_no_N = G_hat_inv @ Omega_hat @ G_hat_inv.T # (G_hat')^{-1} = (G_hat^{-1})'\n",
    "V_cov_beta_hat = (1/N_gmm) * V_cov_beta_hat_no_N\n",
    "\n",
    "# 7. Hipótese Nula H0: beta_k = 0, beta_l = 0\n",
    "beta_gmm_estimated = np.array([beta_k_gmm, beta_l_gmm])\n",
    "\n",
    "# 8. Estatística de Wald (Chi-quadrado)\n",
    "# W = beta_hat' * [V_cov(beta_hat)]^-1 * beta_hat\n",
    "V_cov_beta_hat_inv = np.linalg.inv(V_cov_beta_hat)\n",
    "W_stat = beta_gmm_estimated.T @ V_cov_beta_hat_inv @ beta_gmm_estimated\n",
    "\n",
    "# 9. Estatística F de Wald\n",
    "F_stat = W_stat / q_restrictions\n",
    "\n",
    "# 10. P-valor para a estatística F\n",
    "# Graus de liberdade: q_restrictions (numerador), N_gmm - K_params (denominador)\n",
    "p_value_F = 1 - f_dist.cdf(F_stat, q_restrictions, N_gmm - K_params)\n",
    "\n",
    "print(\"\\nTeste de Wald para significância conjunta de beta_k e beta_l (H0: beta_k=0, beta_l=0):\")\n",
    "print(f\"   Número de observações (GMM): {N_gmm}\")\n",
    "print(f\"   Matriz G_hat:\\n{G_hat}\")\n",
    "print(f\"   Matriz Omega_hat:\\n{Omega_hat}\")\n",
    "print(f\"   Matriz de Variância-Covariância (V_cov_beta_hat):\\n{V_cov_beta_hat}\")\n",
    "print(f\"   Estatística de Wald (Chi^2({q_restrictions})): {W_stat:.4f}\")\n",
    "print(f\"   Estatística F({q_restrictions}, {N_gmm - K_params}): {F_stat:.4f}\")\n",
    "print(f\"   P-valor (F): {p_value_F:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195b47d",
   "metadata": {},
   "source": [
    "3. Calcule o erro-padrão robusto:\n",
    "\n",
    " $ SE^*_l = \\sqrt{\\frac{1}{n}(\\hat{Avar}(\\hat{\\delta}(\\hat{W})))} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b914f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Erros Padrão Robustos:\n",
      "   SE(beta_k): 0.0608\n",
      "   SE(beta_l): 0.2767\n",
      "\n",
      "Testes de Significância Individual (usando erros padrão robustos):\n",
      "   beta_k: Coef = 0.3618, SE = 0.0608, t = 5.95, p-valor = 0.0000\n",
      "   beta_l: Coef = -0.1739, SE = 0.2767, t = -0.63, p-valor = 0.5297\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# Os erros padrão robustos são a raiz quadrada da diagonal da matriz de Var-Cov\n",
    "se_beta_k = np.sqrt(V_cov_beta_hat[0, 0])\n",
    "se_beta_l = np.sqrt(V_cov_beta_hat[1, 1])\n",
    "\n",
    "print(\"\\nErros Padrão Robustos:\")\n",
    "print(f\"   SE(beta_k): {se_beta_k:.4f}\")\n",
    "print(f\"   SE(beta_l): {se_beta_l:.4f}\")\n",
    "\n",
    "# Estatísticas t e p-valores para significância individual\n",
    "t_beta_k = beta_k_gmm / se_beta_k\n",
    "t_beta_l = beta_l_gmm / se_beta_l\n",
    "\n",
    "# P-valores (usando distribuição normal padrão para grandes amostras)\n",
    "p_value_beta_k = 2 * (1 - norm.cdf(np.abs(t_beta_k)))\n",
    "p_value_beta_l = 2 * (1 - norm.cdf(np.abs(t_beta_l)))\n",
    "\n",
    "print(\"\\nTestes de Significância Individual (usando erros padrão robustos):\")\n",
    "print(f\"   beta_k: Coef = {beta_k_gmm:.4f}, SE = {se_beta_k:.4f}, t = {t_beta_k:.2f}, p-valor = {p_value_beta_k:.4f}\")\n",
    "print(f\"   beta_l: Coef = {beta_l_gmm:.4f}, SE = {se_beta_l:.4f}, t = {t_beta_l:.2f}, p-valor = {p_value_beta_l:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6eb604",
   "metadata": {},
   "source": [
    "4. Calcule as variáveis de transparência de Andrews, Gentzkow e Shapiro (2017, 2020). A medida de sensitividade (AGS 2017):\n",
    "\n",
    "$ \\hat{\\Lambda} = -(\\hat{G}(\\hat{\\theta})' \\hat{W} \\hat{G}(\\hat{\\theta}))^{-1} \\hat{G}(\\hat{\\theta})' \\hat{W} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623eebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Matriz Lambda_hat:\n",
      "[[ -1.08647887   4.93864725]\n",
      " [  5.88363349 -23.88290421]]\n",
      "\n",
      "Interpretação:\n",
      "  - Cada linha corresponde a um parâmetro (beta_k, beta_l).\n",
      "  - Cada coluna corresponde a uma condição de momento (E[u*k]=0, E[u*l_lag]=0).\n",
      "  - Exemplo: Lambda[0, 0] = -1.0865 indica que um aumento de 1 unidade\n",
      "    no valor do primeiro momento (relativo ao capital) mudaria a estimativa de beta_k\n",
      "    em -1.0865 unidades.\n"
     ]
    }
   ],
   "source": [
    "G_hat_inv = np.linalg.inv(G_hat)\n",
    "Lambda_hat = -G_hat_inv\n",
    "print(f\"   Matriz Lambda_hat:\\n{Lambda_hat}\")\n",
    "\n",
    "print(\"\\nInterpretação:\")\n",
    "print(\"  - Cada linha corresponde a um parâmetro (beta_k, beta_l).\")\n",
    "print(\"  - Cada coluna corresponde a uma condição de momento (E[u*k]=0, E[u*l_lag]=0).\")\n",
    "print(f\"  - Exemplo: Lambda[0, 0] = {Lambda_hat[0, 0]:.4f} indica que um aumento de 1 unidade\")\n",
    "print(\"    no valor do primeiro momento (relativo ao capital) mudaria a estimativa de beta_k\")\n",
    "print(f\"    em {Lambda_hat[0, 0]:.4f} unidades.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
